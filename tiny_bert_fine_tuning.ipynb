{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/purusoni/miniconda3/envs/youtube_classifier_extention/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_mapped_csv_path = Path.cwd() / \"labeled_mapped.csv\"\n",
    "\n",
    "labels_to_ids = {\n",
    "    \"Educational\": 0,\n",
    "    \"Entertainment\": 1,\n",
    "}\n",
    "\n",
    "num_labels = len(labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['label', 'title', 'path']\n",
      "Dataframe shape: (2736, 3)\n",
      "\n",
      "Elements per label before making them equal:\n",
      "label\n",
      "Entertainment    1825\n",
      "Educational       911\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Elements per label after making them equal:\n",
      "label\n",
      "Entertainment    911\n",
      "Educational      911\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataframe shape: (1822, 3)\n",
      "Dataframe head:\n",
      "           label                                              title   path\n",
      "0  Entertainment  SIDEMEN GUESS THE LINK OFFENSIVE EDITION by Mo...  1.jpg\n",
      "1  Entertainment  Our Most Demonic Experience  Zak Bagans Haunte...  2.jpg\n",
      "2  Entertainment  How SpaceX and Boeing will get Astronauts to t...  3.jpg\n",
      "3  Entertainment  'TIRED OF IT' Voters sound off after latest at...  4.jpg\n",
      "4    Educational  Can We Predict An Outbreak's Future - Modeling...  5.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV file\n",
    "df = pd.read_csv(labeled_mapped_csv_path)\n",
    "\n",
    "# Read header for column names\n",
    "column_names = df.columns.tolist()\n",
    "print(\"Column names:\", column_names)\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "\n",
    "# Print number of elements for each label\n",
    "print()\n",
    "print(\"Elements per label before making them equal:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Only retain equal number of elements for each label\n",
    "least_number_of_elements = df['label'].value_counts().min()\n",
    "df = df.groupby('label').head(least_number_of_elements).reset_index(drop=True)\n",
    "\n",
    "# Print number of elements for each label\n",
    "print()\n",
    "print(\"Elements per label after making them equal:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print()\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "print(\"Dataframe head:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of a title in the Dataframe: 135\n"
     ]
    }
   ],
   "source": [
    "print(\"Max length of a title in the Dataframe:\", df['title'].map(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entertainment', 'Entertainment', 'Entertainment', 'Entertainment', 'Educational']\n",
      "[1 1 1 1 0]\n",
      "['SIDEMEN GUESS THE LINK OFFENSIVE EDITION by MoreSidemen', 'Our Most Demonic Experience  Zak Bagans Haunted Museum by Sam and Colby', 'How SpaceX and Boeing will get Astronauts to the ISS by EverydayAstronaut', \"'TIRED OF IT' Voters sound off after latest attacks on Trump by FoxNews\", \"Can We Predict An Outbreak's Future - Modeling Crash Course Outbreak Science #9 by crashcourse\"]\n",
      "Lenght of dataset:  1822\n"
     ]
    }
   ],
   "source": [
    "# Split data into labels and titles\n",
    "all_labels = df[\"label\"].tolist()\n",
    "all_titles = df[\"title\"].tolist()\n",
    "\n",
    "# Print first 5 all_labels and all_titles\n",
    "print(all_labels[:5])\n",
    "\n",
    "# Convert all_labels to integers\n",
    "all_labels = np.vectorize(labels_to_ids.get)(all_labels)\n",
    "print(all_labels[:5])\n",
    "\n",
    "# all_labels = to_categorical(all_labels)\n",
    "# print(all_labels[:10])\n",
    "\n",
    "print(all_titles[:5])\n",
    "print(\"Lenght of dataset: \", len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 18:07:50.125162: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2023-08-16 18:07:50.125205: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-08-16 18:07:50.125211: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-08-16 18:07:50.125255: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-08-16 18:07:50.125277: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['fit_denses.0.bias', 'fit_denses.2.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.1.bias', 'fit_denses.3.bias', 'fit_denses.2.weight', 'fit_denses.4.bias', 'fit_denses.3.weight', 'fit_denses.0.weight']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  14350248  \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  626       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14350874 (54.74 MB)\n",
      "Trainable params: 14350874 (54.74 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "tiny_bert_model = TFBertForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', from_pt = True, num_labels=num_labels)\n",
    "tiny_bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_val, sentences_test, labels_train_val, labels_test = train_test_split(all_titles, all_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "[101, 4748, 14945, 4335, 2189, 5702, 2189, 2005, 2488, 6693, 1998, 3579, 1010, 2817, 2189, 2011, 2665, 5596, 5453, 1011, 19613, 2189, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_val_encodings = tokenizer(sentences_train_val, truncation=True, padding=True, max_length=MAX_LEN)\n",
    "test_encodings = tokenizer(sentences_test, truncation=True, padding=True, max_length=MAX_LEN)\n",
    "\n",
    "print(train_val_encodings.keys())\n",
    "print(train_val_encodings['input_ids'][0])\n",
    "print(train_val_encodings['attention_mask'][0])\n",
    "\n",
    "train_val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_val_encodings),\n",
    "    labels_train_val\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    labels_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length:  1093\n",
      "Validation dataset length:  364\n",
      "Test dataset length:  365\n"
     ]
    }
   ],
   "source": [
    "valid_size = int(0.25 * len(train_val_dataset)) \n",
    "\n",
    "val_dataset = train_val_dataset.take(valid_size) \n",
    "train_dataset = train_val_dataset.skip(valid_size) \n",
    "\n",
    "print(\"Train dataset length: \", len(list(train_dataset)))\n",
    "print(\"Validation dataset length: \", len(list(val_dataset)))\n",
    "print(\"Test dataset length: \", len(list(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['fit_denses.0.bias', 'fit_denses.2.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.1.bias', 'fit_denses.3.bias', 'fit_denses.2.weight', 'fit_denses.4.bias', 'fit_denses.3.weight', 'fit_denses.0.weight']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 18:07:54.685121: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - ETA: 0s - loss: 0.6640 - accuracy: 0.6615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 18:08:21.456498: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 34s 105ms/step - loss: 0.6640 - accuracy: 0.6615 - val_loss: 0.5693 - val_accuracy: 0.8022\n",
      "Epoch 2/200\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 0.4966 - accuracy: 0.8124 - val_loss: 0.4107 - val_accuracy: 0.8626\n",
      "Epoch 3/200\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 0.3588 - accuracy: 0.8783 - val_loss: 0.3658 - val_accuracy: 0.8599\n",
      "Epoch 4/200\n",
      "274/274 [==============================] - 25s 89ms/step - loss: 0.2802 - accuracy: 0.9122 - val_loss: 0.3546 - val_accuracy: 0.8681\n",
      "Epoch 5/200\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 0.2264 - accuracy: 0.9323 - val_loss: 0.3676 - val_accuracy: 0.8791\n",
      "Epoch 6/200\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 0.1915 - accuracy: 0.9478 - val_loss: 0.3887 - val_accuracy: 0.8599\n",
      "Epoch 7/200\n",
      "274/274 [==============================] - 25s 89ms/step - loss: 0.1528 - accuracy: 0.9625 - val_loss: 0.3916 - val_accuracy: 0.8681\n",
      "Epoch 8/200\n",
      "274/274 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9607Restoring model weights from the end of the best epoch: 4.\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 0.1446 - accuracy: 0.9607 - val_loss: 0.3868 - val_accuracy: 0.8654\n",
      "Epoch 8: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tinybert_fine_tuned/tokenizer_config.json',\n",
       " 'tinybert_fine_tuned/special_tokens_map.json',\n",
       " 'tinybert_fine_tuned/vocab.txt',\n",
       " 'tinybert_fine_tuned/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', from_pt = True, num_labels=2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=4,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_dataset.batch(4), \n",
    "    epochs=200, \n",
    "    batch_size=4, \n",
    "    validation_data=val_dataset.batch(4), \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "model.save_pretrained('tinybert_fine_tuned')\n",
    "tokenizer.save_pretrained('tinybert_fine_tuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  14350248  \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  626       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14350874 (54.74 MB)\n",
      "Trainable params: 14350874 (54.74 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mhistory\n\u001b[1;32m      4\u001b[0m figure, axis \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m axis[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m axis[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m axis[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_title(\u001b[39m'\u001b[39m\u001b[39mTraining and Validation Loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the training process\n",
    "history = model.history\n",
    "\n",
    "figure, axis = plt.subplots(1, 2, figsize=(16, 5))\n",
    "axis[0].plot(history.history['loss'], label='Training Loss')\n",
    "axis[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axis[0].set_title('Training and Validation Loss')\n",
    "axis[0].set_xlabel('Epochs')\n",
    "axis[0].set_ylabel('Loss')\n",
    "axis[0].legend()\n",
    "\n",
    "axis[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axis[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axis[1].set_title('Training and Validation Accuracy')\n",
    "axis[1].set_xlabel('Epochs')\n",
    "axis[1].set_ylabel('Accuracy')\n",
    "axis[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at tinybert_fine_tuned were not used when initializing TFBertForSequenceClassification: ['dropout_27']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at tinybert_fine_tuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 18:11:31.415484: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 10s 43ms/step - loss: 0.4013 - accuracy: 0.8356\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model = TFBertForSequenceClassification.from_pretrained('tinybert_fine_tuned')\n",
    "tokenizer = BertTokenizer.from_pretrained('tinybert_fine_tuned')\n",
    "model.compile(metrics=['accuracy'])\n",
    "\n",
    "print('Evaluate on test data')\n",
    "results = model.evaluate(test_dataset.batch(2), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at tinybert_fine_tuned were not used when initializing TFBertForSequenceClassification: ['dropout_27']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at tinybert_fine_tuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Calculate time taken to process one title\n",
    "# 1. Load model\n",
    "# 2. Load tokenizer\n",
    "# 3. Load title\n",
    "# 4. Tokenize title\n",
    "# 5. Predict\n",
    "# 6. Calculate time taken\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('tinybert_fine_tuned')\n",
    "tokenizer = BertTokenizer.from_pretrained('tinybert_fine_tuned')\n",
    "model.compile(metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 169ms/step\n",
      "Time taken to predict: 0.19353485107421875\n",
      "Title:  How to bake a simple homemade minecraft server\n",
      "Prediction:  Entertainment\n"
     ]
    }
   ],
   "source": [
    "title = \"How to bake a simple homemade minecraft server\"\n",
    "\n",
    "start_time = time.time()\n",
    "title_encoding = tokenizer(title, truncation=True, padding=True, max_length=MAX_LEN)\n",
    "prediction = model.predict([title_encoding['input_ids'], title_encoding['attention_mask']])\n",
    "prediction = np.argmax(prediction[0], axis=1)[0]\n",
    "print(\"Time taken to predict:\", time.time() - start_time)\n",
    "print(\"Title: \", title)\n",
    "print(\"Prediction: \", [\"Education\", \"Entertainment\"][prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube_classifier_extention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
